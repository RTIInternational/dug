#############################################################
##
## Dug is semantic search framework for digging in dark data
##
##   Ingest:
##
##     link: Annotate a data set with ontology identifiers
##           based on NLP and named entity recognition.
##       usage: bin/dug link <metadata>
##
##     load: Turn the data into a knowledge graph and
##           insert it into a database.
##       usage: bin/dug load <metadata>_tagged.json
##
##   Crawl & Index:
##
##     crawl: Execute graph queries against an aggregator.
##            Record the knowledge graphs in a cache.
##       usage: bin/dug crawl
##
##     index: Analyze the graph to build a search index.
##       usage: bin/dug index
##
##     query: Query the search engine from the command line
##       usage: bin/dug query <text>
##
##   Search API:
##
##     api: Provides a REST API to the search engine.
##       bin/dug api [--debug] [--port=<int>]
##
##   Development:
##
##     stack: Run search engine, neo4j, redis, and the
##            search OpenAPI endpoint.
##
##       usage: bin/dug stack [service ]*
##
##     dev init: Run once before any services to generate
##         docker/.env containing passwords, etc.
##       usage: bin/dug dev init
##
##     dev conf: Is run automatically in this script to
##         source docker/.env and make env variables
##         available to all client applications.
##       usage: bin/dug dev conf
##
##     test: Run automated functional tests.
##       usage: bin/dug test
##
#############################################################
#!/bin/bash

# Configure the PYTHONPATH
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
DUG_HOME=$( dirname $DIR )
export PYTHONPATH=$DUG_HOME:$DUG_HOME/kgx

# Ensure the KGX library is installed
if [ ! -d $DUG_HOME/kgx ]; then
    git clone https://github.com/NCATS-Tangerine/kgx.git $DUG_HOME/kgx
fi

# Development tools.
dev () {
    # This must be run once and only once before running the stack.
    # It generates a docker-compose environment file, including
    # passwords for the services.
    DUGENV=$DUG_HOME/docker/.env
    init () {
        HOSTNAME=$HOSTNAME RANDOM=$RANDOM envsubst < $DUG_HOME/docker/.env.template > $DUGENV
    }
    # This must be run before each time a component connects to the stack.
    # It reads configuration settings for the stack, including passwords
    # and sets them as environment variables so the application will have access to them.
    conf ()  {
        if [ ! -f $DUGENV ]; then
            echo missing $DUGENV - will probably need that.
            exit 0 #return
        fi
        source $DUGENV
        export $(cut -d= -f1 $DUGENV)
    }
    $*
}

if [ "$(echo $* | grep -c 'dev init')" == 0 ]; then
   echo setting up dev
   dev conf
fi

### INGEST:

#############################################################
##
## Link: Read metadata and annotate to create linked data.
##
#############################################################
link () {
    if [ $(echo "$*" | grep -c "_variables_") == 1 ]; then
        ELASTIC_API_HOST=localhost python -m  dug.ingest --tagged $*
    else
        python -m dug.ingest --annotate $*
    fi
}

#############################################################
##
## Load: Create knowledge graph and store in a database.
##
#############################################################
load () {
    python -m dug.ingest --load $input $*
}

### Crawl/Index

#############################################################
##
## Crawl: Gather knowledge graphs via queries.
##
#############################################################
crawl () {
    curdir=$PWD
    cd $DUG_HOME/dug
    python -m dug.core --crawl $*
    cd $curdir
}

#############################################################
##
## Index: Create indices and add to search engine.
##
#############################################################
index () {
    curdir=$PWD
    cd $DUG_HOME/dug
    python -m dug.core --index $*
    cd $curdir
}

#############################################################
##
## Query: Test the created index with a query.
##
#############################################################
query () {
    python -m dug.core --query $*
}

#############################################################
##
## API: Run the OpenAPI search endpoint.
##
#############################################################
api () {
    python -m dug.api $*
}

#############################################################
##
## Stack: Start the system's essential services.
##
#############################################################
stack () {
    docker-compose -f $DUG_HOME/docker/docker-compose.yaml up $*
}

#############################################################
##
## Test: Automated functional test
##
#############################################################
test () {

     # Prerequisites: stack must be running.
     # API must be running.

     # Delete the test index
     curl -X DELETE http://elastic:$ELASTIC_PASSWORD@$ELASTIC_API_HOST:9200/test

     # Ingest, annotate, load knowledge graph.
     bin/dug link data/dd.xml
     bin/dug load data/dd_tagged.json

     # Crawl model queries, create index, test index.
     bin/dug crawl
     bin/dug index

     # The best format for variables we have is currently
     # implemented so that, if we pass the additional
     # --index <arg> flag, it will use the neo4j
     # database to drive queries, create knoweldge graphs,
     # create documents, and index those for search.
     bin/dug link data/topmed_variables_v1.0.csv --index x
     sleep 4
     bin/dug query coug # direct to search engine

     # Query via API
     bin/dug query_api coug
     bin/dug query_api copd
     bin/dug query_api sleep
}

#############################################################
##
## Query_API: Query the index via the search API.
##
#############################################################
query_api () {
     query="`echo '{"index" : "test", "query" : "'$1'"}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search
}

$*

exit 0
