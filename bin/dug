#############################################################
##
## Dug is a semantic search pipeline framework including
##
##   Ingest:
##
##     link: Annotate a data set with ontology identifiers
##           based on NLP and other tagging mechanisms.
##       usage: bin/dug link <metadata>
##
##     load: Turn the data into a knowledge graph into a
##           graph and add insert it into a database. 
##       usage: bin/dug load <metadata>_tagged.json
##
##   Crawl & Index:
##
##     crawl: Execute graph queries against an aggregator.
##            Record the knowledge graphs in a cache.
##       usage: bin/dug crawl
##
##     index: Analyze the graph to build a search index.
##       usage: bin/dug index
##
##     query: Query the search engine from the command line
##       usage: bin/dug query <text>
##
##   Search:
##
##     api: Provides a REST API to the search engine.
##       bin/dug api [--debug] [--port=<int>]
##
#############################################################
#!/bin/bash

# Configure the PYTHONPATH
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
DUG_HOME=$( dirname $DIR )
export PYTHONPATH=$DUG_HOME:$DUG_HOME/kgx

# Ensure the KGX library is installed
if [ ! -d $DUG_HOME/kgx ]; then
    git clone https://github.com/NCATS-Tangerine/kgx.git $DUG_HOME/kgx
fi

### INGEST:

#############################################################
##
## Link: Generate linked data information from raw metadata.
##
#############################################################
link () {
    python -m dug.ingest --annotate $*
}

#############################################################
##
## Load: Create a knowledge graph in a graph database.
##
#############################################################
load () {
    python -m dug.ingest --load $input $*
}

### Crawl/Index

#############################################################
##
## Crawl: Gather knowledge graphs via queries.
##
#############################################################
crawl () {
    curdir=$PWD
    cd $DUG_HOME/dug
    python -m dug.core --crawl $*
    cd $curdir
}

#############################################################
##
## Index: Create indices and add to search engine.
##
#############################################################
index () {
    curdir=$PWD
    cd $DUG_HOME/dug
    python -m dug.core --index $*
    cd $curdir
}

#############################################################
##
## Query: Test the created index with a query.
##
#############################################################
query () {
    python -m dug.core --query $*
}

#############################################################
##
## Query: Test the created index with a query.
##
#############################################################
api () {
    python -m dug.api $*
}

#############################################################
##
## Stack: Start the system's essential persistence services.
##
#############################################################
stack () {
      set -x
      docker-compose -f $DUG_HOME/docker/docker-compose.yaml up
      set +x
}
#############################################################
##
## Test: Automated functional test
##
#############################################################
test () {

     # Prerequisites: stack must be running.
     # API must be running.

     # Delete the test index
     curl -X DELETE http://localhost:9200/test

     # Ingest, annotate, load knowledge graph.
     bin/dug link data/dd.xml
     bin/dug load data/dd_tagged.json

     # Crawl model queries, create index, test index.
     bin/dug crawl
     bin/dug index
     bin/dug query coug # direct to search engine

     # Query via API
     bin/dug query_api coug | grep MONDO:0015118
}
query_api () {
      curl --data '{"index" : "test", "query" : { "match" : {"name": {"query" : "coug", "fuzziness" : 1 }}}}' \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search
}

$*

exit 0
