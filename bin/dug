#############################################################
##
## Dug is semantic search framework for digging in dark data
##
##   Ingest:
##
##     link: Annotate a data set with ontology identifiers
##           based on NLP and named entity recognition.
##       usage: bin/dug link <metadata>
##
##     load: Turn the data into a knowledge graph and
##           insert it into a database.
##       usage: bin/dug load <metadata>_tagged.json
##
##   Crawl & Index:
##
##     crawl: Execute graph queries against an aggregator.
##            Record the knowledge graphs in a cache.
##       usage: bin/dug crawl
##
##     index: Analyze the graph to build a search index.
##       usage: bin/dug index
##
##     query: Query the search engine from the command line
##       usage: bin/dug query <text>
##
##   Search API:
##
##     api: Provides a REST API to the search engine.
##       bin/dug api [--debug] [--port=<int>]
##
##   Development:
##
##     stack: Run search engine, neo4j, redis, and the
##            search OpenAPI endpoint.
##
##       usage: bin/dug stack [service ]*
##
##     dev init: Run once before any services to generate
##         docker/.env containing passwords, etc.
##       usage: bin/dug dev init
##
##     dev conf: Is run automatically in this script to
##         source docker/.env and make env variables
##         available to all client applications.
##       usage: bin/dug dev conf
##
##     test: Run automated functional tests.
##       usage: bin/dug test
##
#############################################################
#!/bin/bash

# Configure the PYTHONPATH
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
DUG_HOME=$( dirname $DIR )
DUGENV=$DUG_HOME/docker/.env
export PYTHONPATH=$DUG_HOME:$DUG_HOME/kgx

# for debugging
set -x

# Ensure the KGX library is installed
if [ ! -d $DUG_HOME/kgx ]; then
    git clone https://github.com/NCATS-Tangerine/kgx.git $DUG_HOME/kgx
fi

# Development environment.
dev () {
    init () {
    	if [[ ! -f $DUGENV ]]; then
            # This must be run once and only once before running the stack.
            # It generates a docker-compose environment file, including
            # passwords for the services. This is used when executing docker-compose.
	    echo "generate .env file..."
	    HOSTNAME=$HOSTNAME RANDOM=$RANDOM envsubst < $DUG_HOME/docker/.env.template > $DUGENV

            # The .env.template used above specifies service hostnames consistent with the ones
            # used inside the docker-compose specification. Here, we create a parallel environment
            # file with hostnames all set to localhost but keeping other settings the same. This allows
            # tools we run in development to connect to services running in compose.
	    echo "generate .env.dev file..."
            cat $DUGENV | sed -e "s,neo4j,localhost,g" \
                              -e "s,elasticsearch,localhost,g" \
                              -e "s,redis,localhost,g" > $DUGENV.dev
	fi
        source $DUGENV.dev
        export $(cut -d= -f1 $DUGENV.dev)
    }
    $*
}

#############################################################
##
## Clean: Delete ES indices
##
#############################################################
clean () {
    curdir=$PWD
    cd $DUG_HOME/dug
    python -m dug.core --clean
    cd $curdir
}

### INGEST:

#############################################################
##
## Link: Read metadata and annotate to create linked data.
##
#############################################################
link () {
    if [ $(echo "$*" | grep -c "_variables_") == 1 ]; then
        ELASTIC_API_HOST=${ELASTIC_API_HOST-"localhost"} python -m dug.ingest --tagged $*
    else
        python -m dug.ingest --annotate $*
    fi
}

#############################################################
##
## Load: Create knowledge graph and store in a database.
##
#############################################################
load () {
    python -m dug.ingest --load $input $*
}

### Crawl/Index

#############################################################
##
## Index: Create indices and add to search engine.
##
#############################################################
index () {
    curdir=$PWD
    cd $DUG_HOME/dug
    python -m dug.core --index $*
    cd $curdir
}

#############################################################
##
## Crawl: Gather knowledge graphs from TranQL, create indices, and add to search engine.
##
#############################################################
crawl () {
    python -m dug.core --crawl $*
}

#############################################################
##
## Crawl_by_tag: Gather knowledge graphs from TranQL, organize by tag, create indices, and add to search engine.
##
#############################################################
crawl_by_tag () {
    python -m dug.core --crawl-by-tag $*
}

#############################################################
##
## Crawl_by_tag: Gather knowledge graphs from TranQL, organize by tag, create indices, and add to search engine.
##
#############################################################
crawl_by_concept () {
    python -m dug.core --crawl-by-concept $*
}

#############################################################
##
## Query: Test the created index with a query.
##
#############################################################
query () {
    python -m dug.core --query $*
}

#############################################################
##
## QueryKG: Test the created knowledge graph index with a query.
##
#############################################################
query_kg () {
    python -m dug.core --kg-id $1 --query-kg ${@:2}
}

#############################################################
##
## API: Run the OpenAPI search endpoint.
##
#############################################################
api () {
    python -m dug.api $*
}

#############################################################
##
## Stack: Start the system's essential services.
##
#############################################################
stack () {
    source $DUGENV
    export $(cut -d= -f1 $DUGENV)

    if [[ ! -f $REDIS_DATA/appendonly.aof  ]]; then
      echo "Initializing redis with baseline AOF..."
      cp $DUG_HOME/data/redis/appendonly.aof $REDIS_DATA/
    fi

    if [[ ! -d $DUG_HOME/dug-search-client ]]; then
      echo "Initializing DUG search client..."
      git clone --single-branch --branch concept_group_ui https://github.com/helxplatform/dug-search-client.git $DUG_HOME/dug-search-client
    fi

    CLIENT_PORT=80 docker-compose -f $DUG_HOME/dug-search-client/docker-compose.yaml $* &

    HOSTNAME=$HOSTNAME docker-compose \
	    --env-file $DUG_HOME/docker/.env \
	    -f $DUG_HOME/docker/docker-compose.yaml -p dug $*
}

#############################################################
##
## Test: Automated functional test
##
#############################################################
test () {

    set -e
    # Prerequisites:
    #   * Stack must be running via "bin/dug stack".
    #   * If running from a host pointing at docker-compose, must have
    #     a docker/.env.dev file with all hostnames set to localhost.

    # Delete the test index
    curl -X DELETE http://elastic:$ELASTIC_PASSWORD@$ELASTIC_API_HOST:9200/test
    curl -X DELETE http://elastic:$ELASTIC_PASSWORD@$ELASTIC_API_HOST:9200/test_kg

    # Ingest, annotate, load knowledge graph.
    bin/dug link data/dd.xml
    bin/dug load data/dd_tagged.json

    # The best format for variables we have is currently
    # implemented so that, if we pass the additional
    # --index <arg> flag, it will use the neo4j
    # database to drive queries, create knoweldge graphs,
    # create documents, and index those for search.
    bin/dug link data/topmed_variables_v1.0.csv --index x
    sleep 4

    bin/dug query coug # direct to search engine

    # Query via API
    bin/dug query_api coug
    bin/dug query_api copd
    bin/dug query_api sleep

    set +e
}

#############################################################
##
## Python Tests: Run the python unit tests for DUG
##
#############################################################
pytests () {
  set -e

  # Create the virtualenv if it doesn't exist already and
  # install the required modules to the venv
  if [[ ! -d $DUG_HOME/venv ]]; then
    # TODO: Check that python3 exists before trying to use it
    python3 -m venv $DUG_HOME/venv
    source $DUG_HOME/venv/bin/activate
    pip install -r $DUG_HOME/requirements.txt
  fi

  # NOTE: In certain scenarios we'll run the venv/bin/activate
  # script twice, which is okay, since it's idempotent
  source $DUG_HOME/venv/bin/activate
  cd $DUG_HOME/dug/tests && python3 -m pytest
  deactivate && cd $DUG_HOME

  set +e
}

#############################################################
##
## Query_API: Query the index via the search API.
##
#############################################################
query_api () {
     query="`echo '{"index" : "concepts_index", "query" : "'$*'"}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search
}

#############################################################
##
## Query_NBoost: Query the index via the search API, but
##               using nboost.
##
#############################################################
query_nboost () {
     query="`echo '{"index" : "test", "query" : "'$*'", "boosted": 1}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search
}


#############################################################
##
## Query_KG_API: Query the knowledge-graph index via the search API.
##
#############################################################
query_kg_api () {
     unique_id=$1
     query="`echo '{"index" : "kg_index", "query" : "'${@:2}'", "unique_id" : "'$unique_id'"}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search_kg
}

dev init

$*

exit 0
